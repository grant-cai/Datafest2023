{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc05ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.278, 'neu': 0.603, 'pos': 0.119, 'compound': -0.9987}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"strike again.csv\")\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg = []\n",
    "neu = []\n",
    "pos = []\n",
    "compound = []\n",
    "sample_size = []\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b69798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.28, 'neu': 0.547, 'pos': 0.173, 'compound': -0.0516}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"strike again.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1107_1113 = data[(data['Date']>=\"2022-11-07\") & (data['Date']<\"2022-11-13\") & (data['School'] == 'UCSB')]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1107_1113[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1107_1113['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8170fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.233, 'neu': 0.636, 'pos': 0.131, 'compound': -0.9794}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"strike again.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1114_1120 = data[(data['Date']>=\"2022-11-14\") & (data['Date']<\"2022-11-20\") & (data['School'] == 'UCSB')]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1114_1120[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1114_1120['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c33769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.183, 'neu': 0.648, 'pos': 0.169, 'compound': -0.0943}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"strike again.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1121_1127 = data[(data['Date']>=\"2022-11-21\") & (data['Date']<\"2022-11-27\") & (data['School'] == 'UCSB')]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1121_1127[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1121_1127['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35e2283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.241, 'neu': 0.669, 'pos': 0.091, 'compound': -0.8447}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"strike again.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1128_1204 = data[(data['Date']>=\"2022-11-28\") & (data['Date']<\"2022-12-04\") & (data['School'] == 'UCSB')]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1128_1204[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1128_1204['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c2d3f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.405, 'neu': 0.527, 'pos': 0.068, 'compound': -0.9403}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"strike again.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1205_1211 = data[(data['Date']>=\"2022-12-05\") & (data['Date']<\"2022-12-11\") & (data['School'] == 'UCSB')]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1205_1211[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1205_1211['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e049de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.291, 'neu': 0.648, 'pos': 0.06, 'compound': -0.9653}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"strike again.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1212_1218 = data[(data['Date']>=\"2022-12-12\") & (data['Date']<\"2022-12-18\") & (data['School'] == 'UCSB')]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1212_1218[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1212_1218['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16d7a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.243, 'neu': 0.757, 'pos': 0.0, 'compound': -0.5377}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"strike again.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1219_1225 = data[(data['Date']>=\"2022-12-19\") & (data['Date']<\"2022-12-25\") & (data['School'] == 'UCSB')]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1219_1225[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1219_1225['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "054829eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg: [0.28, 0.233, 0.183, 0.241, 0.405, 0.291, 0.243]\n",
      "Neu: [0.547, 0.636, 0.648, 0.669, 0.527, 0.648, 0.757]\n",
      "Pos: [0.173, 0.131, 0.169, 0.091, 0.068, 0.06, 0.0]\n",
      "Compound: [-0.0516, -0.9794, -0.0943, -0.8447, -0.9403, -0.9653, -0.5377]\n",
      "Sample Size: [17, 74, 6, 25, 15, 27, 6]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Neg: {neg}\\nNeu: {neu}\\nPos: {pos}\\nCompound: {compound}\\nSample Size: {sample_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31cf25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
